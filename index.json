[{"content":"","date":null,"permalink":"/tags/conference-paper/","section":"Tags","summary":"","title":"Conference-Paper"},{"content":" We propose DragText, which optimizes text embedding in conjunction with the dragging process to pair with the modified image embedding. Abstract #Point-based image editing enables accurate and flexible control through content dragging. However, the role of text embedding in the editing process has not been thoroughly investigated. A significant aspect that remains unexplored is the interaction between text and image embeddings.\nIn this study, we show that during the progressive editing of an input image in a diffusion model, the text embedding remains constant. As the image embedding increasingly diverges from its initial state, the discrepancy between the image and text embeddings presents a significant challenge. Moreover, we found that the text prompt significantly influences the dragging process, particularly in maintaining content integrity and achieving the desired manipulation.\nTo utilize these insights, we propose DragText, which optimizes text embedding in conjunction with the dragging process to pair with the modified image embedding. Simultaneously, we regularize the text optimization process to preserve the integrity of the original text prompt. Our approach can be seamlessly integrated with existing diffusion-based drag methods with only a few lines of code.\nMethod # During the edit, the original image embedding \\( \\mathbf{z}_t \\) naturally deviates to the dragged image latent vector \\( \\bar{\\mathbf{z}}_t \\). With no text optimization, the corresponding text embedding \\(\\mathbf{c}\\) is decoupled from \\( \\bar{\\mathbf{z}}_t \\). Hence, optimal text embedding \\(\\hat{\\mathbf{c}} \\)coupled with dragged images has to be acquired to make the optimal latent vector \\( \\hat{\\mathbf{z}}_t \\)which then holds the related semantics via text. In DragText, the image \\(\\mathbf{x}_0\\) is mapped to a low-dimensional space through a VAE encoder, and the text is encoded by a CLIP text encoder as the text embedding \\(\\mathbf{c}\\). Through DDIM inversion with \\(\\mathbf{c}\\), the latent vector \\( \\mathbf{z}_t \\) is obtained. At time step \\(t=35\\), \\( \\mathbf{z}_t^0 \\) and \\(\\mathbf{c}\\) are optimized to \\( \\hat{\\mathbf{z}}_t^k \\) and \\(\\hat{\\mathbf{c}} \\) by iterating with motion supervision (M.S.) and point tracking (P.T.) \\(k\\)-times. Citation #@article{choi2024dragtext, title={DragText: Rethinking Text Embedding in Point-based Image Editing}, author={Choi, Gayoon and Jeong, Taejin and Hong, Sujung and Joo, Jaehoon and Hwang, Seong Jae}, journal={arXiv preprint arXiv:2407.17843}, year={2024} } ","date":"31 August 2024","permalink":"/papers/dragtext2025/","section":"Publication","summary":"","title":"DragText: Rethinking Text Embedding in Point-based Image Editing"},{"content":"","date":null,"permalink":"/papers/","section":"Publication","summary":"","title":"Publication"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/wacv/","section":"Tags","summary":"","title":"Wacv"},{"content":" Hi! I\u0026rsquo;m Gayoon Choi and currently pursuing my M.S. degree at Yonsei University, advised by Prof. Seong Jae Hwang. My research interests include Image Editing, Parameter-Efficient Fine Tuning, and Test-Time Adaptation. News üéâ # [Aug 2024] One Paper EARLY ACCEPTED for WACV 2025 [May 2024] One Paper Accepted for MICCAI 2024 [Sep 2023] Join MICV Lab in Yonsei Univeristy for M.S. Degree Publications # DragText: Rethinking Text Embedding in Point-based Image Editing Gayoon Choi, Taejin Jeong, Sujung Hong, Jaehoon Joo, Seong Jae Hwang WACV 2025 Paper arXiv Code Project Page Parameter Efficient Fine Tuning for Multi-scanner PET to PET Reconstruction Yumin Kim*, Gayoon Choi*, Seong Jae Hwang MICCAI 2024 Paper arXiv Code Project Page ","date":null,"permalink":"/","section":"Welcome to my Blog üê∂","summary":"","title":"Welcome to my Blog üê∂"},{"content":"","date":null,"permalink":"/tags/miccai/","section":"Tags","summary":"","title":"Miccai"},{"content":" We introduce PETITE, Parameter-Efficient Fine-Tuning for MultI-scanner PET to PET REconstruction which represents the optimal PEFT combination when independently applying encoder-decoder components to each model architecture. Abstract #Reducing scan time in Positron Emission Tomography (PET) imaging while maintaining high-quality images is crucial for minimizing patient discomfort and radiation exposure. Due to the limited size of datasets and distribution discrepancy across scanners in medical imaging, fine-tuning in a parameter-efficient and effective manner is on the rise.\nMotivated by the potential of Parameter-Efficient Fine-Tuning (PEFT), we aim to address these issues by effectively leveraging PEFT to improve limited data and GPU resource issues in multi-scanner setups. In this paper, we introduce PETITE, Parameter-Efficient Fine-Tuning for MultI-scanner PET to PET REconstruction which represents the optimal PEFT combination when independently applying encoder-decoder components to each model architecture.\nTo the best of our knowledge, this study is the first to systematically explore the efficacy of diverse PEFT techniques in medical imaging reconstruction tasks via prevalent encoder-decoder models. This investigation, in particular, brings intriguing insights into PETITE as we show further improvements by treating encoder and decoder separately and mixing different PEFT methods, namely, Mix-PEFT.\nUsing multi-scanner PET datasets comprised of five different scanners, we extensively test the cross-scanner PET scan time reduction performances (i.e., a model pre-trained on one scanner is fine-tuned on a different scanner) of 21 feasible Mix-PEFT combinations to derive optimal PETITE. We show that training with less than 1% parameters using PETITE performs on par with full fine-tuning (i.e., 100% parameter).\nMethod # The overview of PETITE: Scheme for single source-target settings in PET scan time reduction with PEFT. We leverage the PEFT methodology in a medical reconstruction task to reduce the scan time of PET images on scanners with different dimensions, voxel spacing, and institutions. To the best of our knowledge, this extensive study represents the first application of the PEFT methodology within the field of medical imaging. The pipeline of the encoder-decoder structure of each ViT-based model. (a) 3D CVT-GAN features a generator with a ViT-based encoder and decoder. Only the first three layers of the encoder and the first two layers of the decoder are trained. (b) UNETR consists of a ViT-based encoder and a CNN-based decoder. The optimal PEFT combination independently applying encoder-decoder components to each model architecture. We provide novel insights into the optimal PEFT settings tailored for the reconstruction model. Upon experimenting with possible Mix-PEFT, we found that using less than 1% of parameters can achieve performance comparable to Full-FT, carefully considering encoder and decoder architecture. Citation #@article{kim2024parameter, title={Parameter Efficient Fine Tuning for Multi-scanner PET to PET Reconstruction}, author={Kim, Yumin and Choi, Gayoon and Hwang, Seong Jae}, journal={arXiv preprint arXiv:2407.07517}, year={2024} } ","date":"18 June 2024","permalink":"/papers/petite2024/","section":"Publication","summary":"","title":"PETITE:Parameter Efficient Fine Tuning for Multi-scanner PET to PET Reconstruction"},{"content":"","date":null,"permalink":"/tags/competition/","section":"Tags","summary":"","title":"Competition"},{"content":"","date":null,"permalink":"/projects/","section":"Project Experience","summary":"","title":"Project Experience"},{"content":" This is an attempt to combine the video generation model with ImageBind to create a pipeline that can generate video even with multimodal input. News # [Dec 2023] Won Novelty Prize for the 3rd YAICON, Yonsei Artificial Intelligence ","date":"11 December 2023","permalink":"/projects/vggnet2023/","section":"Project Experience","summary":"","title":"Video Graphic Generation Network"},{"content":"","date":null,"permalink":"/tags/incubator-program/","section":"Tags","summary":"","title":"Incubator-Program"},{"content":" We developed an automated medicine envelop and prescription recognition with pharmaceutial curation service using OCR Data from medication and cosmetic packaging, and scraped medicine envelope image datasets News # [Apr 2023] Funded by IHEI Workstation, Yonsei University ","date":"1 April 2023","permalink":"/projects/pharmavision2023/","section":"Project Experience","summary":"","title":"PharmaVision"},{"content":" We developed a service for predicting BCS using pet image data and determining appropriate food portions. News # [Jan 2023] Won Outstanding Project for the final presentation of the IHEI Workstation, Yonsei University [Dec 2022] Won the ü•áGrand Prize at the 2022 AI Learning Data Hackathon for Pet Dogs and Cats Health Information, MSIT and MIA [Jul 2022] Won Outstanding Project for the midterm presentation of the IHEI Workstation, Yonsei University [Apr 2022] Funded by IHEI Workstation, Yonsei University ","date":"28 December 2022","permalink":"/projects/healthcare2022/","section":"Project Experience","summary":"","title":"Multimodal Pet Health Care Service"},{"content":" About Gayoon Choiüôå Education #M.S. in Artificial Intelligence (Sep 2023 - Aug 2025) #Yonsei University\nAdvisor: Prof. Seong Jae Hwang\nB.A. in Economics (Mar 2018 - Aug 2023) #Yonsei University\nMinor Degree: Computer Science and Engineering, Applied Statistics\nGPA: 3.76 / 4.5\nResearch Experience #Medical Imaging \u0026amp; Computer Vision Lab (Oct 2022 - Aug 2023) #Yonsei University\nAdvisor: Prof. Seong Jae Hwang\nShort-time PET Reconstruction in Parameter Efficient Fine Tuning Aspect\nConference Reviewer # IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025 Teaching Experience # Teaching Assitant, Yonsei University, Spring 2024 Honors and Awards # Novelty Awards, Video Graphic Generation Network, YAICON, 2023 1st Awards, Multimodal Pet Health Care Service, AI Data Utilization Hackathon, 2022 ","date":null,"permalink":"/bio/","section":"Welcome to my Blog üê∂","summary":"","title":"Bio"},{"content":"","date":null,"permalink":"/posts/","section":"Blog Posts","summary":"","title":"Blog Posts"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]